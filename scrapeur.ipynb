{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs des catégories à scraper\n",
    "CATEGORIES = {\n",
    "    \"emploi\": \"https://www.expat-dakar.com/emploi\",\n",
    "    \"terrains_a_vendre\": \"https://www.expat-dakar.com/terrains-a-vendre\",\n",
    "   # \"accessoires_multimedia\": \"https://www.expat-dakar.com/multimedia\",\n",
    "   # \"decoration_linge_de_maison\": \"https://www.expat-dakar.com/decoration-linge-de-maison\"\n",
    "   # \"véhicules\": \"https://www.expat-dakar.com/vehicules\"\n",
    "   # \"maison\": \"https://www.expat-dakar.com/maison\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dossier pour stocker les images\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F de télécharger les images\n",
    "def download_image(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du téléchargement de l'image : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour scraper une catégorie\n",
    "def scrape_category(category_name, category_url):\n",
    "    page = 1\n",
    "    data = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scraping {category_name}, page {page}...\")\n",
    "        url = f\"{category_url}?page={page}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # chercher toutes les annonces sur la page\n",
    "        listings = soup.find_all(\"div\", class_=\"listings-cards__list-item\")\n",
    "        if not listings:\n",
    "            break\n",
    "\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                # Extraction titre\n",
    "                title_element = listing.find(\"div\", class_=\"listing-card__header__title\")\n",
    "                title = title_element.text.strip() if title_element else \"Non spécifié\"\n",
    "\n",
    "                # Extraction prix\n",
    "                price_element = listing.find(\"span\", class_=\"listing-card__price__value\")\n",
    "                price = price_element.text.strip() if price_element else \"Non spécifié\"\n",
    "\n",
    "                # Extraction de la loca\n",
    "                location_element = listing.find(\"div\", class_=\"listing-card__header__location\")\n",
    "                location = location_element.text.strip() if location_element else \"Non spécifié\"\n",
    "\n",
    "                # Extraire l'état du produit\n",
    "                condition_element = listing.find(\"span\", class_=\"listing-card__header__tags__item--condition\")\n",
    "                condition = condition_element.text.strip() if condition_element else \"Non spécifié\"\n",
    "\n",
    "                # Extraction dex image\n",
    "                image_element = listing.find(\"img\", class_=\"listing-card__image__resource\")\n",
    "                image_url = image_element[\"src\"] if image_element else None\n",
    "\n",
    "                # Nommation et télécharger desimages\n",
    "                image_filename = f\"images/{category_name}_{page}_{listings.index(listing)}.jpg\"\n",
    "                if image_url:\n",
    "                    download_image(image_url, image_filename)\n",
    "\n",
    "                # Ajouter les données à la liste\n",
    "                data.append({\n",
    "                    \"Catégorie\": category_name,\n",
    "                    \"Titre\": title,\n",
    "                    \"Prix\": price,\n",
    "                    \"Localisation\": location,\n",
    "                    \"État\": condition,\n",
    "                    \"Image\": image_filename if image_url else \"Aucune image\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de l'extraction des données d'une annonce : {e}\")\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping emploi, page 1...\n",
      "Scraping emploi, page 2...\n",
      "Scraping emploi, page 3...\n",
      "Scraping emploi, page 4...\n",
      "Scraping emploi, page 5...\n",
      "Scraping emploi, page 6...\n",
      "Scraping emploi, page 7...\n",
      "Scraping emploi, page 8...\n",
      "Scraping emploi, page 9...\n",
      "Scraping emploi, page 10...\n",
      "Scraping emploi, page 11...\n",
      "Scraping emploi, page 12...\n",
      "Scraping emploi, page 13...\n",
      "Scraping emploi, page 14...\n",
      "Scraping emploi, page 15...\n",
      "Scraping emploi, page 16...\n",
      "Scraping emploi, page 17...\n",
      "Scraping emploi, page 18...\n",
      "Scraping emploi, page 19...\n",
      "Scraping emploi, page 20...\n",
      "Scraping terrains_a_vendre, page 1...\n",
      "Scraping terrains_a_vendre, page 2...\n",
      "Scraping terrains_a_vendre, page 3...\n",
      "Scraping terrains_a_vendre, page 4...\n",
      "Scraping terrains_a_vendre, page 5...\n",
      "Scraping terrains_a_vendre, page 6...\n",
      "Scraping terrains_a_vendre, page 7...\n",
      "Scraping terrains_a_vendre, page 8...\n",
      "Scraping terrains_a_vendre, page 9...\n",
      "Scraping terrains_a_vendre, page 10...\n",
      "Scraping terrains_a_vendre, page 11...\n",
      "Scraping terrains_a_vendre, page 12...\n",
      "Scraping terrains_a_vendre, page 13...\n",
      "Scraping terrains_a_vendre, page 14...\n",
      "Scraping terrains_a_vendre, page 15...\n",
      "Scraping terrains_a_vendre, page 16...\n",
      "Scraping terrains_a_vendre, page 17...\n",
      "Scraping terrains_a_vendre, page 18...\n",
      "Scraping terrains_a_vendre, page 19...\n",
      "Scraping terrains_a_vendre, page 20...\n",
      "Scraping terrains_a_vendre, page 21...\n",
      "Scraping terrains_a_vendre, page 22...\n",
      "Scraping terrains_a_vendre, page 23...\n",
      "Scraping terrains_a_vendre, page 24...\n",
      "Scraping terrains_a_vendre, page 25...\n",
      "Scraping terrains_a_vendre, page 26...\n",
      "Scraping terrains_a_vendre, page 27...\n",
      "Scraping terrains_a_vendre, page 28...\n",
      "Scraping terrains_a_vendre, page 29...\n",
      "Scraping terrains_a_vendre, page 30...\n",
      "Scraping terrains_a_vendre, page 31...\n",
      "Scraping terrains_a_vendre, page 32...\n",
      "Scraping terrains_a_vendre, page 33...\n",
      "Scraping terrains_a_vendre, page 34...\n",
      "Scraping terrains_a_vendre, page 35...\n",
      "Scraping terrains_a_vendre, page 36...\n",
      "Scraping terrains_a_vendre, page 37...\n",
      "Scraping terrains_a_vendre, page 38...\n",
      "Scraping terrains_a_vendre, page 39...\n",
      "Scraping terrains_a_vendre, page 40...\n",
      "Scraping terrains_a_vendre, page 41...\n",
      "Scraping terrains_a_vendre, page 42...\n",
      "Scraping terrains_a_vendre, page 43...\n",
      "Scraping terrains_a_vendre, page 44...\n",
      "Scraping terrains_a_vendre, page 45...\n",
      "Scraping terrains_a_vendre, page 46...\n",
      "Scraping terrains_a_vendre, page 47...\n",
      "Scraping terrains_a_vendre, page 48...\n",
      "Scraping terminé.\n"
     ]
    }
   ],
   "source": [
    "# Scrapeur de toutes les catégories\n",
    "all_data = []\n",
    "for category_name, category_url in CATEGORIES.items():\n",
    "    all_data.extend(scrape_category(category_name, category_url))\n",
    "    \n",
    "# Sauvegarde des données \n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_excel(\"annonces_categories.xlsx\", index=False)\n",
    "\n",
    "print(\"Scraping terminé.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
